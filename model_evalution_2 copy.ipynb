{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# PyTorch libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchsummary import summary\n",
    "\n",
    "# Utilities\n",
    "from IPython.display import display\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Custom modules\n",
    "from preprocessing_utils import get_class_names, get_policies, get_datasets, get_transform\n",
    "from training_utils import predict_model, load_model, plot_roc_curve\n",
    "from dataset import CheXpertDataset\n",
    "\n",
    "# LIME (Local Interpretable Model-agnostic Explanations)\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "import lime.lime_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = get_transform()\n",
    "\n",
    "# Define the class names for the medical conditions\n",
    "class_names = get_class_names()\n",
    "\n",
    "# Define the policies for dataset preparation\n",
    "policies = get_policies()\n",
    "\n",
    "# Path to the zip file\n",
    "zip_path = \"./chexpert.zip\"\n",
    "\n",
    "# Path to the directory where the datasets will be extracted\n",
    "data_dir = \"./CheXpert-v1.0-small\"\n",
    "\n",
    "# Set the device to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Set the batch size\n",
    "batch_size = 16\n",
    "\n",
    "# Set pandas display options to show all columns\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Load the datasets\n",
    "train_df, validation_df, test_df = get_datasets(zip_path)\n",
    "\n",
    "if train_df is None or validation_df is None or test_df is None:\n",
    "    print(\"Error loading the datasets.\")\n",
    "else:\n",
    "    print(\"Datasets loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "path",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Pleural Effusion",
         "rawType": "int32",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "d377f83d-1c67-4a1d-8a36-5cc6c27ae420",
       "rows": [
        [
         "0",
         "CheXpert-v1.0-small/valid/patient64735/study1/view1_frontal.jpg",
         "0"
        ],
        [
         "1",
         "CheXpert-v1.0-small/valid/patient64634/study1/view1_frontal.jpg",
         "0"
        ],
        [
         "2",
         "CheXpert-v1.0-small/valid/patient64712/study1/view1_frontal.jpg",
         "0"
        ],
        [
         "3",
         "CheXpert-v1.0-small/valid/patient64662/study1/view1_frontal.jpg",
         "0"
        ],
        [
         "4",
         "CheXpert-v1.0-small/valid/patient64578/study1/view1_frontal.jpg",
         "0"
        ],
        [
         "5",
         "CheXpert-v1.0-small/valid/patient64633/study1/view1_frontal.jpg",
         "1"
        ],
        [
         "6",
         "CheXpert-v1.0-small/valid/patient64617/study1/view1_frontal.jpg",
         "0"
        ],
        [
         "7",
         "CheXpert-v1.0-small/valid/patient64597/study1/view1_frontal.jpg",
         "0"
        ],
        [
         "8",
         "CheXpert-v1.0-small/valid/patient64569/study1/view1_frontal.jpg",
         "0"
        ],
        [
         "9",
         "CheXpert-v1.0-small/valid/patient64545/study1/view1_frontal.jpg",
         "1"
        ],
        [
         "10",
         "CheXpert-v1.0-small/valid/patient64586/study1/view1_frontal.jpg",
         "0"
        ],
        [
         "11",
         "CheXpert-v1.0-small/valid/patient64709/study1/view1_frontal.jpg",
         "0"
        ],
        [
         "12",
         "CheXpert-v1.0-small/valid/patient64575/study1/view1_frontal.jpg",
         "0"
        ],
        [
         "13",
         "CheXpert-v1.0-small/valid/patient64636/study1/view1_frontal.jpg",
         "1"
        ],
        [
         "14",
         "CheXpert-v1.0-small/valid/patient64551/study1/view1_frontal.jpg",
         "0"
        ],
        [
         "15",
         "CheXpert-v1.0-small/valid/patient64574/study1/view1_frontal.jpg",
         "0"
        ],
        [
         "16",
         "CheXpert-v1.0-small/valid/patient64655/study1/view1_frontal.jpg",
         "1"
        ],
        [
         "17",
         "CheXpert-v1.0-small/valid/patient64558/study1/view1_frontal.jpg",
         "0"
        ],
        [
         "18",
         "CheXpert-v1.0-small/valid/patient64721/study1/view1_frontal.jpg",
         "0"
        ],
        [
         "19",
         "CheXpert-v1.0-small/valid/patient64726/study1/view1_frontal.jpg",
         "0"
        ],
        [
         "20",
         "CheXpert-v1.0-small/valid/patient64653/study1/view1_frontal.jpg",
         "1"
        ],
        [
         "21",
         "CheXpert-v1.0-small/valid/patient64661/study1/view1_frontal.jpg",
         "0"
        ],
        [
         "22",
         "CheXpert-v1.0-small/valid/patient64657/study1/view1_frontal.jpg",
         "1"
        ],
        [
         "23",
         "CheXpert-v1.0-small/valid/patient64556/study1/view1_frontal.jpg",
         "0"
        ],
        [
         "24",
         "CheXpert-v1.0-small/valid/patient64728/study1/view1_frontal.jpg",
         "0"
        ],
        [
         "25",
         "CheXpert-v1.0-small/valid/patient64568/study1/view1_frontal.jpg",
         "0"
        ],
        [
         "26",
         "CheXpert-v1.0-small/valid/patient64675/study1/view1_frontal.jpg",
         "0"
        ],
        [
         "27",
         "CheXpert-v1.0-small/valid/patient64706/study1/view1_frontal.jpg",
         "0"
        ],
        [
         "28",
         "CheXpert-v1.0-small/valid/patient64595/study1/view1_frontal.jpg",
         "0"
        ],
        [
         "29",
         "CheXpert-v1.0-small/valid/patient64580/study1/view1_frontal.jpg",
         "1"
        ],
        [
         "30",
         "CheXpert-v1.0-small/valid/patient64571/study1/view1_frontal.jpg",
         "1"
        ],
        [
         "31",
         "CheXpert-v1.0-small/valid/patient64666/study1/view1_frontal.jpg",
         "0"
        ],
        [
         "32",
         "CheXpert-v1.0-small/valid/patient64567/study1/view1_frontal.jpg",
         "0"
        ],
        [
         "33",
         "CheXpert-v1.0-small/valid/patient64687/study1/view1_frontal.jpg",
         "0"
        ],
        [
         "34",
         "CheXpert-v1.0-small/valid/patient64629/study1/view1_frontal.jpg",
         "0"
        ],
        [
         "35",
         "CheXpert-v1.0-small/valid/patient64606/study1/view1_frontal.jpg",
         "0"
        ],
        [
         "36",
         "CheXpert-v1.0-small/valid/patient64623/study1/view1_frontal.jpg",
         "1"
        ],
        [
         "37",
         "CheXpert-v1.0-small/valid/patient64638/study1/view1_frontal.jpg",
         "0"
        ],
        [
         "38",
         "CheXpert-v1.0-small/valid/patient64724/study1/view1_frontal.jpg",
         "0"
        ],
        [
         "39",
         "CheXpert-v1.0-small/valid/patient64637/study1/view1_frontal.jpg",
         "0"
        ],
        [
         "40",
         "CheXpert-v1.0-small/valid/patient64644/study1/view1_frontal.jpg",
         "1"
        ],
        [
         "41",
         "CheXpert-v1.0-small/valid/patient64583/study1/view1_frontal.jpg",
         "1"
        ],
        [
         "42",
         "CheXpert-v1.0-small/valid/patient64733/study1/view1_frontal.jpg",
         "0"
        ],
        [
         "43",
         "CheXpert-v1.0-small/valid/patient64713/study1/view1_frontal.jpg",
         "0"
        ],
        [
         "44",
         "CheXpert-v1.0-small/valid/patient64598/study1/view1_frontal.jpg",
         "0"
        ],
        [
         "45",
         "CheXpert-v1.0-small/valid/patient64554/study1/view1_frontal.jpg",
         "0"
        ],
        [
         "46",
         "CheXpert-v1.0-small/valid/patient64590/study1/view1_frontal.jpg",
         "0"
        ],
        [
         "47",
         "CheXpert-v1.0-small/valid/patient64559/study1/view1_frontal.jpg",
         "0"
        ],
        [
         "48",
         "CheXpert-v1.0-small/valid/patient64649/study1/view1_frontal.jpg",
         "1"
        ],
        [
         "49",
         "CheXpert-v1.0-small/valid/patient64579/study1/view1_frontal.jpg",
         "0"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 202
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>Pleural Effusion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CheXpert-v1.0-small/valid/patient64735/study1/...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CheXpert-v1.0-small/valid/patient64634/study1/...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CheXpert-v1.0-small/valid/patient64712/study1/...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CheXpert-v1.0-small/valid/patient64662/study1/...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CheXpert-v1.0-small/valid/patient64578/study1/...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>CheXpert-v1.0-small/valid/patient64672/study1/...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>CheXpert-v1.0-small/valid/patient64676/study1/...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>CheXpert-v1.0-small/valid/patient64611/study1/...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>CheXpert-v1.0-small/valid/patient64679/study1/...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>CheXpert-v1.0-small/valid/patient64577/study1/...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>202 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  path  Pleural Effusion\n",
       "0    CheXpert-v1.0-small/valid/patient64735/study1/...                 0\n",
       "1    CheXpert-v1.0-small/valid/patient64634/study1/...                 0\n",
       "2    CheXpert-v1.0-small/valid/patient64712/study1/...                 0\n",
       "3    CheXpert-v1.0-small/valid/patient64662/study1/...                 0\n",
       "4    CheXpert-v1.0-small/valid/patient64578/study1/...                 0\n",
       "..                                                 ...               ...\n",
       "197  CheXpert-v1.0-small/valid/patient64672/study1/...                 0\n",
       "198  CheXpert-v1.0-small/valid/patient64676/study1/...                 1\n",
       "199  CheXpert-v1.0-small/valid/patient64611/study1/...                 0\n",
       "200  CheXpert-v1.0-small/valid/patient64679/study1/...                 0\n",
       "201  CheXpert-v1.0-small/valid/patient64577/study1/...                 1\n",
       "\n",
       "[202 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Check if the dataset is already extracted, if not extract it\n",
    "if not os.path.exists(data_dir):\n",
    "    print(\"Extracting the dataset...\")\n",
    "    #unzip\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(data_dir)\n",
    "    #list files in the extracted dir\n",
    "    os.listdir(data_dir)\n",
    "else:\n",
    "    print(\"Dataset already extracted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction of ground truth samples (4 images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Image ",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "x_start",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "y_start ",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "x_end ",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "y_end",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "x_start.1",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "y_start .1",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_end .1",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "y_end.1",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "7d66751b-a4dd-4275-81c6-3e13030e0e93",
       "rows": [
        [
         "0",
         "Test ",
         "48",
         "160",
         "144",
         "288",
         null,
         null,
         null,
         null
        ],
        [
         "1",
         "Rad_1 ",
         "176",
         "128",
         "288",
         "256",
         null,
         null,
         null,
         null
        ],
        [
         "2",
         "Rad_2",
         "32",
         "128",
         "144",
         "256",
         null,
         null,
         null,
         null
        ],
        [
         "3",
         "Rad_3 ",
         "32",
         "144",
         "144",
         "288",
         "176.0",
         "144.0",
         "288.0",
         "272.0"
        ],
        [
         "4",
         "Rad_5 ",
         "32",
         "160",
         "144",
         "240",
         null,
         null,
         null,
         null
        ]
       ],
       "shape": {
        "columns": 9,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image</th>\n",
       "      <th>x_start</th>\n",
       "      <th>y_start</th>\n",
       "      <th>x_end</th>\n",
       "      <th>y_end</th>\n",
       "      <th>x_start.1</th>\n",
       "      <th>y_start .1</th>\n",
       "      <th>x_end .1</th>\n",
       "      <th>y_end.1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Test</td>\n",
       "      <td>48</td>\n",
       "      <td>160</td>\n",
       "      <td>144</td>\n",
       "      <td>288</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Rad_1</td>\n",
       "      <td>176</td>\n",
       "      <td>128</td>\n",
       "      <td>288</td>\n",
       "      <td>256</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Rad_2</td>\n",
       "      <td>32</td>\n",
       "      <td>128</td>\n",
       "      <td>144</td>\n",
       "      <td>256</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rad_3</td>\n",
       "      <td>32</td>\n",
       "      <td>144</td>\n",
       "      <td>144</td>\n",
       "      <td>288</td>\n",
       "      <td>176.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>288.0</td>\n",
       "      <td>272.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Rad_5</td>\n",
       "      <td>32</td>\n",
       "      <td>160</td>\n",
       "      <td>144</td>\n",
       "      <td>240</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Image   x_start  y_start   x_end   y_end  x_start.1  y_start .1  x_end .1  \\\n",
       "0   Test        48       160     144    288        NaN         NaN       NaN   \n",
       "1  Rad_1       176       128     288    256        NaN         NaN       NaN   \n",
       "2   Rad_2       32       128     144    256        NaN         NaN       NaN   \n",
       "3  Rad_3        32       144     144    288      176.0       144.0     288.0   \n",
       "4  Rad_5        32       160     144    240        NaN         NaN       NaN   \n",
       "\n",
       "   y_end.1  \n",
       "0      NaN  \n",
       "1      NaN  \n",
       "2      NaN  \n",
       "3    272.0  \n",
       "4      NaN  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = r\"ground_truth_data\\bbox320x320.xlsx\"\n",
    "ground_truth_df = pd.read_excel(path)\n",
    "ground_truth_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(320, 320)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Open an image file\n",
    "image = Image.open(\"ground_truth_images/Rad_1.png\")\n",
    "image_array = np.array(image)\n",
    "image_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_df = ground_truth_df[ground_truth_df['Image '] != 'Test ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Image ",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "x_start",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "y_start ",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "x_end ",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "y_end",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "x_start.1",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "y_start .1",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_end .1",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "y_end.1",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "1d394299-9764-455f-9125-18d2de78b351",
       "rows": [
        [
         "1",
         "Rad_1 ",
         "176",
         "128",
         "288",
         "256",
         null,
         null,
         null,
         null
        ],
        [
         "2",
         "Rad_2",
         "32",
         "128",
         "144",
         "256",
         null,
         null,
         null,
         null
        ],
        [
         "3",
         "Rad_3 ",
         "32",
         "144",
         "144",
         "288",
         "176.0",
         "144.0",
         "288.0",
         "272.0"
        ],
        [
         "4",
         "Rad_5 ",
         "32",
         "160",
         "144",
         "240",
         null,
         null,
         null,
         null
        ]
       ],
       "shape": {
        "columns": 9,
        "rows": 4
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image</th>\n",
       "      <th>x_start</th>\n",
       "      <th>y_start</th>\n",
       "      <th>x_end</th>\n",
       "      <th>y_end</th>\n",
       "      <th>x_start.1</th>\n",
       "      <th>y_start .1</th>\n",
       "      <th>x_end .1</th>\n",
       "      <th>y_end.1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Rad_1</td>\n",
       "      <td>176</td>\n",
       "      <td>128</td>\n",
       "      <td>288</td>\n",
       "      <td>256</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Rad_2</td>\n",
       "      <td>32</td>\n",
       "      <td>128</td>\n",
       "      <td>144</td>\n",
       "      <td>256</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rad_3</td>\n",
       "      <td>32</td>\n",
       "      <td>144</td>\n",
       "      <td>144</td>\n",
       "      <td>288</td>\n",
       "      <td>176.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>288.0</td>\n",
       "      <td>272.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Rad_5</td>\n",
       "      <td>32</td>\n",
       "      <td>160</td>\n",
       "      <td>144</td>\n",
       "      <td>240</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Image   x_start  y_start   x_end   y_end  x_start.1  y_start .1  x_end .1  \\\n",
       "1  Rad_1       176       128     288    256        NaN         NaN       NaN   \n",
       "2   Rad_2       32       128     144    256        NaN         NaN       NaN   \n",
       "3  Rad_3        32       144     144    288      176.0       144.0     288.0   \n",
       "4  Rad_5        32       160     144    240        NaN         NaN       NaN   \n",
       "\n",
       "   y_end.1  \n",
       "1      NaN  \n",
       "2      NaN  \n",
       "3    272.0  \n",
       "4      NaN  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_paths = []\n",
    "attributes_list = []\n",
    "\n",
    "for row, column in ground_truth_df.iterrows():\n",
    "    if column['Image '] != \"Test \":\n",
    "        path = f\"ground_truth_images/{column['Image '].strip()}.png\"\n",
    "        images_paths.append(path)\n",
    "        attributes_list.append(column[['Image ', 'x_start', 'y_start ', 'x_end ', 'y_end', 'x_start.1',\n",
    "       'y_start .1', 'x_end .1', 'y_end.1']])\n",
    "        \n",
    "labels = np.array([1, 1, 1, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "path",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "label",
         "rawType": "int32",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "dc8c7c94-e586-449d-8100-027f5ea9407f",
       "rows": [
        [
         "0",
         "ground_truth_images/Rad_1.png",
         "1"
        ],
        [
         "1",
         "ground_truth_images/Rad_2.png",
         "1"
        ],
        [
         "2",
         "ground_truth_images/Rad_3.png",
         "1"
        ],
        [
         "3",
         "ground_truth_images/Rad_5.png",
         "1"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 4
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ground_truth_images/Rad_1.png</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ground_truth_images/Rad_2.png</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ground_truth_images/Rad_3.png</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ground_truth_images/Rad_5.png</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            path  label\n",
       "0  ground_truth_images/Rad_1.png      1\n",
       "1  ground_truth_images/Rad_2.png      1\n",
       "2  ground_truth_images/Rad_3.png      1\n",
       "3  ground_truth_images/Rad_5.png      1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth_images_df = pd.DataFrame({\"path\": images_paths, \"label\": labels})\n",
    "ground_truth_images_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uploading the model we trained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacked model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bar24\\AppData\\Local\\Temp\\ipykernel_39176\\3414834214.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load(\"finetuned_models/stacked_model.pkl\", map_location=device)\n",
      "C:\\Users\\bar24\\AppData\\Local\\Temp\\ipykernel_39176\\3414834214.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(\"finetuned_models/stacked_model.pth\", map_location=device)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = torch.load(\"finetuned_models/stacked_model.pkl\", map_location=device)\n",
    "state_dict = torch.load(\"finetuned_models/stacked_model.pth\", map_location=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## densenet169"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# densenet169 = models.densenet169(weights=models.DenseNet169_Weights.IMAGENET1K_V1)\n",
    "# model = upload_pretrained_densenet169(densenet169, add_layers=True, n_labels=len(class_names), freeze_layers=True)\n",
    "model = torch.load(\"finetuned_models/densenet169.pkl\", map_location=device)\n",
    "state_dict = torch.load(\"finetuned_models/densenet169.pth\", map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataLoader object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CheXpertDataset(test_df, class_names, transform=transform, zip_path=None)\n",
    "\n",
    "# Create DataLoader for the test dataset\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "def evaluate_model(model, test_loader: DataLoader, criterion, device='cuda'):\n",
    "    \"\"\"\n",
    "    Evaluate a trained PyTorch model on a test dataset.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The trained PyTorch model.\n",
    "    - test_loader: DataLoader for the test dataset.\n",
    "    - criterion: The loss function used during training.\n",
    "    - device: Device to evaluate the model on ('cpu' or 'cuda').\n",
    "\n",
    "    Returns:\n",
    "    - test_loss: Average loss on the test dataset.\n",
    "    - test_accuracy: Overall accuracy on the test dataset.\n",
    "    - all_predictions: List of predicted values for all samples.\n",
    "    - all_labels: List of ground-truth labels for all samples.\n",
    "    \"\"\"\n",
    "    # model.to(device)\n",
    "    # model.eval()  # Set the model to evaluation mode\n",
    "    \n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(test_loader, desc=\"Evaluating\", unit=\"batch\"):\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            # Convert outputs to binary predictions (multi-label classification)\n",
    "            predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "            \n",
    "            # Store predictions and labels\n",
    "            all_predictions.append(predicted.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.numel() # Total elements (samples × labels)\n",
    "            \n",
    "    test_loss /= len(test_loader)\n",
    "    test_accuracy = correct / total\n",
    "\n",
    "    # Convert pre`dictions and labels to tensors\n",
    "    all_predictions = torch.cat(all_predictions, dim=0)\n",
    "    all_labels = torch.cat(all_labels, dim=0)\n",
    "\n",
    "    print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "    return test_loss, test_accuracy, all_predictions, all_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the model (densenet121)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "test_loss, test_accuracy, predictions, true_labels = evaluate_model(model, test_loader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we turn test loader into generator so that we could extract images and labels\n",
    "dataiter = iter(test_loader)\n",
    "images, labels = next(dataiter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since batch=16 we extract 16 labels and corresponding images\n",
    "print(f\"images shape: {images.shape} | labels shape: {labels.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from lime.lime_image import LimeImageExplainer\n",
    "from skimage.segmentation import mark_boundaries\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Ensure your model is in evaluation mode and on the proper device.\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "def denormalize2numpy(img_tensor):\n",
    "    # Unnormalize the image: original = normalized * std + mean\n",
    "    # Since mean=0.5 and std=0.5 for grayscale images:\n",
    "    unnormalized = img_tensor * 0.5 + 0.5  #torch.Size([3, 224, 224])\n",
    "    # Convert tensor from (C, H, W) to (H, W, C)\n",
    "    img_np = unnormalized.permute(1, 2, 0).cpu().numpy() #(224, 224, 3)\n",
    "    return img_np\n",
    "\n",
    "# Assuming you have obtained an image from your testloader:\n",
    "# For example, grabbing the first image from a batch:\n",
    "dataiter = iter(test_loader)\n",
    "images, labels = next(dataiter)\n",
    "img = images[5]  # select the first image from the batch\n",
    "img_np = denormalize2numpy(img)\n",
    "\n",
    "# Prediction function for LIME:\n",
    "def predict_fn(images):\n",
    "    \"\"\"\n",
    "    images: list/array of images in NumPy format (H x W x C), with pixel values in [0, 1].\n",
    "    Returns: a NumPy array of shape (n_samples, 2) with class probabilities.\n",
    "    \"\"\"\n",
    "    # Convert each LIME image (numpy array) to a PIL image, then to a tensor.\n",
    "    tensor_images = [\n",
    "        transforms.ToTensor()(Image.fromarray((img * 255).astype(np.uint8)))\n",
    "        for img in images\n",
    "    ]\n",
    "    # Stack into a single batch tensor of shape (N, C, H, W)\n",
    "    batch = torch.stack(tensor_images).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(batch)  # outputs shape: (batch, 1)\n",
    "        # Apply sigmoid to get probability for class 1.\n",
    "        probs_class1 = torch.sigmoid(outputs)\n",
    "        # Create two-class probabilities: class0 = 1 - p, class1 = p.\n",
    "        probs = torch.cat([1 - probs_class1, probs_class1], dim=1)\n",
    "        # print(probs)\n",
    "    \n",
    "    return probs.cpu().numpy()\n",
    "\n",
    "# Create LIME image explainer.\n",
    "explainer = LimeImageExplainer()\n",
    "\n",
    "# Explain the prediction for the chosen image.\n",
    "# For binary classification, we can set top_labels=2.\n",
    "explanation = explainer.explain_instance(\n",
    "    img_np,   \n",
    "    predict_fn,\n",
    "    top_labels=2,              # With binary classification, there are two labels.\n",
    "    hide_color=0,\n",
    "    num_samples=1000           # Number of perturbed samples\n",
    ")\n",
    "\n",
    "# Choose the label you wish to explain. For example, to explain class 1 (positive class):\n",
    "label_to_explain = 0  # or use explanation.top_labels[0] if that suits your needs\n",
    "\n",
    "# Get the explanation mask.\n",
    "temp, mask = explanation.get_image_and_mask(\n",
    "    label=label_to_explain,\n",
    "    positive_only=True,\n",
    "    num_features=1,\n",
    "    hide_rest=False\n",
    ")\n",
    "\n",
    "# Visualize the explanation.\n",
    "plt.imshow(mark_boundaries(temp, mask))\n",
    "plt.title(f\"LIME Explanation for Class {label_to_explain}\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ________________________________________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradcam based on paper: https://arxiv.org/abs/1610.02391\n",
    "#### Library: https://github.com/jacobgil/pytorch-grad-cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_grad_cam import ScoreCAM\n",
    "\n",
    "def apply_gradcam(model, image_path, transform, target_layer=None):\n",
    "    \"\"\"\n",
    "    Apply Grad-CAM to the specified image using the provided DenseNet121 model.\n",
    "    \n",
    "    Args:\n",
    "        model: The trained DenseNet121 model.\n",
    "        image_path: Path to the image for visualization.\n",
    "        transform: Image transformation pipeline.\n",
    "        target_layer: The target convolutional layer for Grad-CAM. If None, uses a default layer.\n",
    "        target_category: (Optional) Target class index for which to compute Grad-CAM.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (resized original image, Grad-CAM heatmap, overlayed image).\n",
    "    \"\"\"\n",
    "    # Ensure the model is in evaluation mode and gradients are enabled\n",
    "    model.eval()\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    # Load the image and get the original image (without resizing)\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    original_img = np.array(img, dtype=np.float32) / 255.0  # Original resolution\n",
    "\n",
    "    # Preprocess the image (this resizes the image to the size expected by the model)\n",
    "    input_tensor = transform(img).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    # If no target layer specified, choose the last conv layer in denseblock4.\n",
    "    if target_layer is None:\n",
    "        try:\n",
    "            # Using the last convolution from denselayer16 in denseblock4       denseblock4{\n",
    "            target_layer = [model.features.denseblock4.denselayer16.conv2] #  (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "                                                                                #} last layer\n",
    "                                                                            # otherwise this error https://github.com/jacobgil/pytorch-grad-cam/issues/393\n",
    "        except AttributeError:\n",
    "            # Fallback: use an alternative convolutional layer\n",
    "            return \"Not possible to extract the required convolutional layer due to missing attribute.\"\n",
    "\n",
    "    # Create GradCAM object\n",
    "    cam = GradCAM(model=model, target_layers=target_layer)\n",
    "\n",
    "    targets = [ClassifierOutputTarget(0)]\n",
    "\n",
    "    \n",
    "    print(targets)\n",
    "\n",
    "    # Generate Grad-CAM heatmap (this triggers the backward pass)\n",
    "    grayscale_cam = cam(input_tensor=input_tensor, targets=targets)[0]\n",
    "    \n",
    "    # print(grayscale_cam.min(), grayscale_cam.max())  # Before normalization\n",
    "    # grayscale_cam = (grayscale_cam - grayscale_cam.min()) / (grayscale_cam.max() - grayscale_cam.min() + 1e-8)\n",
    "    # print(grayscale_cam.min(), grayscale_cam.max())  # After normalization\n",
    "        \n",
    "    # Resize the original image to match the Grad-CAM output dimensions if needed\n",
    "    if original_img.shape[:2] != grayscale_cam.shape: #OTHERWISE  <<ValueError: operands could not be broadcast together with shapes (224,224,3) (320,357,3) THIS SH happens >> \n",
    "        original_img_resized = cv2.resize(original_img, (grayscale_cam.shape[1], grayscale_cam.shape[0])) \n",
    "    else:\n",
    "        original_img_resized = original_img\n",
    "\n",
    "    # Overlay the heatmap on the resized original image\n",
    "    overlay = show_cam_on_image(original_img_resized, grayscale_cam, use_rgb=True) # https://github.com/jacobgil/pytorch-grad-cam/blob/master/pytorch_grad_cam/utils/image.py\n",
    "\n",
    "    return original_img_resized, overlay\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GradCAM implementation for ground truth labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_images_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "for i in range(ground_truth_images_df.shape[0]):\n",
    "    image_path = ground_truth_images_df.iloc[i]['path']\n",
    "    actual_label = ground_truth_images_df.iloc[i][\"label\"]\n",
    "    \n",
    "    # Load BBOX coordinates\n",
    "    x_start, y_start, x_end, y_end, x_start_, y_start_, x_end_, y_end_  = ground_truth_df.iloc[i][['x_start', 'y_start ', 'x_end ', 'y_end', 'x_start.1',\n",
    "       'y_start .1', 'x_end .1', 'y_end.1']]\n",
    "\n",
    "\n",
    "    print(f\"Actual label: {actual_label}\")\n",
    "    \n",
    "    # Only process images with effusion (label == 1)\n",
    "    if actual_label == 1:\n",
    "        original_img, overlay = apply_gradcam(\n",
    "            model=model, \n",
    "            image_path=image_path, \n",
    "            transform=transform,\n",
    "        )\n",
    "        \n",
    "        # Ensure BBOX coordinates are scaled to 224x224\n",
    "        original_width, original_height = Image.open(image_path).size\n",
    "        scale_x = 224 / original_width\n",
    "        scale_y = 224 / original_height\n",
    "        \n",
    "        x_start_rescaled = int(x_start * scale_x)\n",
    "        y_start_rescaled = int(y_start * scale_y)\n",
    "        x_end_rescaled = int(x_end * scale_x)\n",
    "        y_end_rescaled = int(y_end * scale_y)\n",
    "\n",
    "\n",
    "        # HAndle none values\n",
    "        # if not pd.notna(x_start_):\n",
    "        #     print(\"NOT NONE\")\n",
    "        # else:\n",
    "        #     print(\"None\")\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        # Draw BBOX on Original Image\n",
    "        img_with_bbox = original_img.copy()\n",
    "        cv2.rectangle(img_with_bbox, \n",
    "                      (x_start_rescaled, y_start_rescaled), \n",
    "                      (x_end_rescaled, y_end_rescaled), \n",
    "                      color=(255, 255, 255),  \n",
    "                      thickness=1)\n",
    "\n",
    "        # Draw BBOX on Grad-CAM Overlay\n",
    "        overlay_with_bbox = overlay.copy()\n",
    "\n",
    "        cv2.rectangle(overlay_with_bbox, \n",
    "                      (x_start_rescaled, y_start_rescaled), \n",
    "                      (x_end_rescaled, y_end_rescaled), \n",
    "                      color=(255, 255, 255),  \n",
    "                      thickness=1)\n",
    "        \n",
    "        # Display Images\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(img_with_bbox)\n",
    "        plt.title(\"Resized Original Image with BBOX\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(overlay_with_bbox)\n",
    "        plt.title(\"Grad-CAM Heatmap with BBOX\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_filtered = test_df[test_df['Pleural Effusion'] == 1]\n",
    "\n",
    "for i in range(13, 60, 3):\n",
    "    image_path = test_df_filtered.iloc[i]['path']\n",
    "    actual_label = test_df_filtered.iloc[i][\"Pleural Effusion\"]  \n",
    "\n",
    "    print(f\"actual label {actual_label}\")\n",
    "    # Only process images with effusion (label==1)\n",
    "    if actual_label == 1:\n",
    "        original_img, overlay = apply_gradcam(\n",
    "            model=model, \n",
    "            image_path=image_path, \n",
    "            transform=transform,\n",
    "        )\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.imshow(original_img)\n",
    "        plt.title(\"Resized Original Image Pleural Effusion\")\n",
    "        plt.axis(\"off\")\n",
    "        \n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.imshow(overlay)\n",
    "        plt.title(\"Grad-CAM Heatmap Pleual Effusion\")\n",
    "        plt.axis(\"off\")\n",
    "        \n",
    "        \n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ScoreCAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_grad_cam import ScoreCAM\n",
    "\n",
    "def apply_scorecam(model, image_path, transform, target_layer=None):\n",
    "    \"\"\"\n",
    "    Apply Grad-CAM to the specified image using the provided DenseNet121 model.\n",
    "    \n",
    "    Args:\n",
    "        model: The trained DenseNet121 model.\n",
    "        image_path: Path to the image for visualization.\n",
    "        transform: Image transformation pipeline.\n",
    "        target_layer: The target convolutional layer for Grad-CAM. If None, uses a default layer.\n",
    "        target_category: (Optional) Target class index for which to compute Grad-CAM.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (resized original image, Grad-CAM heatmap, overlayed image).\n",
    "    \"\"\"\n",
    "    # Ensure the model is in evaluation mode and gradients are enabled\n",
    "    model.eval()\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    # Load the image and get the original image (without resizing)\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    original_img = np.array(img, dtype=np.float32) / 255.0  # Original resolution\n",
    "\n",
    "    # Preprocess the image (this resizes the image to the size expected by the model)\n",
    "    input_tensor = transform(img).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    # If no target layer specified, choose the last conv layer in denseblock4.\n",
    "    if target_layer is None:\n",
    "        try:\n",
    "            # Using the last convolution from denselayer16 in denseblock4       denseblock4{\n",
    "            target_layer = [model.features.denseblock4.denselayer16.conv2] #  (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "                                                                                #} last layer11ts111\n",
    "                                                                            # otherwise this error https://github.com/jacobgil/pytorch-grad-cam/issues/393\n",
    "        except AttributeError:\n",
    "            # Fallback: use an alternative convolutional layer\n",
    "            return \"Not possible to extract the required convolutional layer due to missing attribute.\"\n",
    "\n",
    "    # Create GradCAM object\n",
    "    cam = ScoreCAM(model=model, target_layers=target_layer)\n",
    "\n",
    "    targets = [ClassifierOutputTarget(0)]\n",
    "\n",
    "    \n",
    "    # print(targets)\n",
    "\n",
    "    # Generate Grad-CAM heatmap (this triggers the backward pass)\n",
    "    grayscale_cam = cam(input_tensor=input_tensor, targets=targets)[0]\n",
    "    \n",
    "    # print(grayscale_cam.min(), grayscale_cam.max())  # Before normalization\n",
    "    # grayscale_cam = (grayscale_cam - grayscale_cam.min()) / (grayscale_cam.max() - grayscale_cam.min() + 1e-8)\n",
    "    # print(grayscale_cam.min(), grayscale_cam.max())  # After normalization\n",
    "        \n",
    "    # Resize the original image to match the Grad-CAM output dimensions if needed\n",
    "    if original_img.shape[:2] != grayscale_cam.shape: #OTHERWISE  <<ValueError: operands could not be broadcast together with shapes (224,224,3) (320,357,3) THIS SH happens >> \n",
    "        original_img_resized = cv2.resize(original_img, (grayscale_cam.shape[1], grayscale_cam.shape[0])) \n",
    "    else:\n",
    "        original_img_resized = original_img\n",
    "\n",
    "    # Overlay the heatmap on the resized original image\n",
    "    overlay = show_cam_on_image(original_img_resized, grayscale_cam, use_rgb=True) # https://github.com/jacobgil/pytorch-grad-cam/blob/master/pytorch_grad_cam/utils/image.py\n",
    "\n",
    "    return original_img_resized, overlay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "for i in range(ground_truth_images_df.shape[0]):\n",
    "    image_path = ground_truth_images_df.iloc[i]['path']\n",
    "    actual_label = ground_truth_images_df.iloc[i][\"label\"]\n",
    "    \n",
    "    # Load BBOX coordinates\n",
    "    x_start, y_start, x_end, y_end, x_start_, y_start_, x_end_, y_end_  = ground_truth_df.iloc[i][['x_start', 'y_start ', 'x_end ', 'y_end', 'x_start.1',\n",
    "       'y_start .1', 'x_end .1', 'y_end.1']]\n",
    "\n",
    "\n",
    "    print(f\"Actual label: {actual_label}\")\n",
    "    \n",
    "    # Only process images with effusion (label == 1)\n",
    "    if actual_label == 1:\n",
    "        original_img, overlay = apply_scorecam(\n",
    "            model=model, \n",
    "            image_path=image_path, \n",
    "            transform=transform,\n",
    "        )\n",
    "        \n",
    "        # Ensure BBOX coordinates are scaled to 224x224\n",
    "        original_width, original_height = Image.open(image_path).size\n",
    "        scale_x = 224 / original_width\n",
    "        scale_y = 224 / original_height\n",
    "        \n",
    "        x_start_rescaled = int(x_start * scale_x)\n",
    "        y_start_rescaled = int(y_start * scale_y)\n",
    "        x_end_rescaled = int(x_end * scale_x)\n",
    "        y_end_rescaled = int(y_end * scale_y)\n",
    "\n",
    "\n",
    "        # HAndle none values\n",
    "        # if not pd.notna(x_start_):\n",
    "        #     print(\"NOT NONE\")\n",
    "        # else:\n",
    "        #     print(\"None\")\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        # Draw BBOX on Original Image\n",
    "        img_with_bbox = original_img.copy()\n",
    "        cv2.rectangle(img_with_bbox, \n",
    "                      (x_start_rescaled, y_start_rescaled), \n",
    "                      (x_end_rescaled, y_end_rescaled), \n",
    "                      color=(255, 255, 255),  \n",
    "                      thickness=1)\n",
    "\n",
    "        # Draw BBOX on Grad-CAM Overlay\n",
    "        overlay_with_bbox = overlay.copy()\n",
    "\n",
    "        cv2.rectangle(overlay_with_bbox, \n",
    "                      (x_start_rescaled, y_start_rescaled), \n",
    "                      (x_end_rescaled, y_end_rescaled), \n",
    "                      color=(255, 255, 255),  \n",
    "                      thickness=1)\n",
    "        \n",
    "        # Display Images\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(img_with_bbox)\n",
    "        plt.title(\"Resized Original Image with BBOX\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(overlay_with_bbox)\n",
    "        plt.title(\"Grad-CAM Heatmap with BBOX\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AblationCAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_grad_cam import AblationCAM\n",
    "\n",
    "def apply_ablationcam(model, image_path, transform, target_layer=None):\n",
    "    \"\"\"\n",
    "    Apply Grad-CAM to the specified image using the provided DenseNet121 model.\n",
    "    \n",
    "    Args:\n",
    "        model: The trained DenseNet121 model.\n",
    "        image_path: Path to the image for visualization.\n",
    "        transform: Image transformation pipeline.\n",
    "        target_layer: The target convolutional layer for Grad-CAM. If None, uses a default layer.\n",
    "        target_category: (Optional) Target class index for which to compute Grad-CAM.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (resized original image, Grad-CAM heatmap, overlayed image).\n",
    "    \"\"\"\n",
    "    # Ensure the model is in evaluation mode and gradients are enabled\n",
    "    model.eval()\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    # Load the image and get the original image (without resizing)\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    original_img = np.array(img, dtype=np.float32) / 255.0  # Original resolution\n",
    "\n",
    "    # Preprocess the image (this resizes the image to the size expected by the model)\n",
    "    input_tensor = transform(img).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    # If no target layer specified, choose the last conv layer in denseblock4.\n",
    "    if target_layer is None:\n",
    "        try:\n",
    "            # Using the last convolution from denselayer16 in denseblock4       denseblock4{\n",
    "            target_layer = [model.features.denseblock4.denselayer16.conv2] #  (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "                                                                                #} last layer11ts111\n",
    "                                                                            # otherwise this error https://github.com/jacobgil/pytorch-grad-cam/issues/393\n",
    "        except AttributeError:\n",
    "            # Fallback: use an alternative convolutional layer\n",
    "            return \"Not possible to extract the required convolutional layer due to missing attribute.\"\n",
    "\n",
    "    # Create GradCAM object\n",
    "    cam = ScoreCAM(model=model, target_layers=target_layer)\n",
    "\n",
    "    targets = [ClassifierOutputTarget(0)]\n",
    "\n",
    "    \n",
    "    # print(targets)\n",
    "\n",
    "    # Generate Grad-CAM heatmap (this triggers the backward pass)\n",
    "    grayscale_cam = cam(input_tensor=input_tensor, targets=targets)[0]\n",
    "    \n",
    "    # print(grayscale_cam.min(), grayscale_cam.max())  # Before normalization\n",
    "    # grayscale_cam = (grayscale_cam - grayscale_cam.min()) / (grayscale_cam.max() - grayscale_cam.min() + 1e-8)\n",
    "    # print(grayscale_cam.min(), grayscale_cam.max())  # After normalization\n",
    "        \n",
    "    # Resize the original image to match the Grad-CAM output dimensions if needed\n",
    "    if original_img.shape[:2] != grayscale_cam.shape: #OTHERWISE  <<ValueError: operands could not be broadcast together with shapes (224,224,3) (320,357,3) THIS SH happens >> \n",
    "        original_img_resized = cv2.resize(original_img, (grayscale_cam.shape[1], grayscale_cam.shape[0])) \n",
    "    else:\n",
    "        original_img_resized = original_img\n",
    "\n",
    "    # Overlay the heatmap on the resized original image\n",
    "    overlay = show_cam_on_image(original_img_resized, grayscale_cam, use_rgb=True) # https://github.com/jacobgil/pytorch-grad-cam/blob/master/pytorch_grad_cam/utils/image.py\n",
    "\n",
    "    return original_img_resized, overlay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "for i in range(ground_truth_images_df.shape[0]):\n",
    "    image_path = ground_truth_images_df.iloc[i]['path']\n",
    "    actual_label = ground_truth_images_df.iloc[i][\"label\"]\n",
    "    \n",
    "    # Load BBOX coordinates\n",
    "    x_start, y_start, x_end, y_end, x_start_, y_start_, x_end_, y_end_  = ground_truth_df.iloc[i][['x_start', 'y_start ', 'x_end ', 'y_end', 'x_start.1',\n",
    "       'y_start .1', 'x_end .1', 'y_end.1']]\n",
    "\n",
    "\n",
    "    print(f\"Actual label: {actual_label}\")\n",
    "    \n",
    "    # Only process images with effusion (label == 1)\n",
    "    if actual_label == 1:\n",
    "        original_img, overlay = apply_ablationcam(\n",
    "            model=model, \n",
    "            image_path=image_path, \n",
    "            transform=transform,\n",
    "        )\n",
    "        \n",
    "        # Ensure BBOX coordinates are scaled to 224x224\n",
    "        original_width, original_height = Image.open(image_path).size\n",
    "        scale_x = 224 / original_width\n",
    "        scale_y = 224 / original_height\n",
    "        \n",
    "        x_start_rescaled = int(x_start * scale_x)\n",
    "        y_start_rescaled = int(y_start * scale_y)\n",
    "        x_end_rescaled = int(x_end * scale_x)\n",
    "        y_end_rescaled = int(y_end * scale_y)\n",
    "\n",
    "\n",
    "        # HAndle none values\n",
    "        # if not pd.notna(x_start_):\n",
    "        #     print(\"NOT NONE\")\n",
    "        # else:\n",
    "        #     print(\"None\")\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        # Draw BBOX on Original Image\n",
    "        img_with_bbox = original_img.copy()\n",
    "        cv2.rectangle(img_with_bbox, \n",
    "                      (x_start_rescaled, y_start_rescaled), \n",
    "                      (x_end_rescaled, y_end_rescaled), \n",
    "                      color=(255, 255, 255),  \n",
    "                      thickness=1)\n",
    "\n",
    "        # Draw BBOX on Grad-CAM Overlay\n",
    "        overlay_with_bbox = overlay.copy()\n",
    "\n",
    "        cv2.rectangle(overlay_with_bbox, \n",
    "                      (x_start_rescaled, y_start_rescaled), \n",
    "                      (x_end_rescaled, y_end_rescaled), \n",
    "                      color=(255, 255, 255),  \n",
    "                      thickness=1)\n",
    "        \n",
    "        # Display Images\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(img_with_bbox)\n",
    "        plt.title(\"Resized Original Image with BBOX\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(overlay_with_bbox)\n",
    "        plt.title(\"Grad-CAM Heatmap with BBOX\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let us extract activations (embeddings) from feature layer in densenet121 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this example, we do it based on ground_truth_images_df\n",
    "\n",
    "https://www.digitalocean.com/community/tutorials/pytorch-hooks-gradient-clipping-debugging#the-forward-hook-for-visualising-activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "\n",
    "class FeatureExtractor:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.model.eval()  # Set model to evaluation mode\n",
    "        self.activations = dict() # here we store activations per sample and then .clear() it\n",
    "        \n",
    "    def hook_fn(self, module, input, output):\n",
    "        self.activations['output'] = output  # Store feature map\n",
    "        \n",
    "    def get_embeddings(self, image_paths, transform):\n",
    "        embeddings = [] \n",
    "        \n",
    "        # Register hook\n",
    "        hook_handle = self.model.features.register_forward_hook(self.hook_fn)\n",
    "        \n",
    "        with torch.no_grad():  # Prevents tracking gradients\n",
    "            for idx, path in enumerate(image_paths):\n",
    "                # Load and preprocess image\n",
    "                image = Image.open(path).convert('RGB')\n",
    "                image_tensor = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "                # Clear previous activations\n",
    "                self.activations.clear()\n",
    "                \n",
    "                # Forward pass\n",
    "                _ = self.model(image_tensor)\n",
    "                \n",
    "                # Retrieve hooked feature maps\n",
    "                features = self.activations['output']\n",
    "                \n",
    "                # Apply ReLU activation\n",
    "                features = F.relu(features, inplace=True)\n",
    "                \n",
    "                # Global average pooling\n",
    "                pooled_features = F.adaptive_avg_pool2d(features, (1, 1))\n",
    "                \n",
    "                # Flatten into a 1D vector\n",
    "                embedding = torch.flatten(pooled_features, 1)\n",
    "                \n",
    "                # Append to list after detaching from computation graph\n",
    "                embeddings.append(embedding.detach().cpu())\n",
    "                \n",
    "                print(f\"Processed image {idx+1}, embedding shape: {embedding.shape}\")\n",
    "        \n",
    "        # Remove hook\n",
    "        hook_handle.remove()\n",
    "        \n",
    "        return torch.cat(embeddings, dim=0)  # Combine all embeddings into one tensor\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let us extract test images we test upon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "folder = \"ranking_annotations\" \n",
    "\n",
    "ranking_image_paths = [\n",
    "    os.path.join(folder, file)\n",
    "    for file in os.listdir(folder) if file.startswith(\"Rad\")] \n",
    "\n",
    "testing_image_path = [\n",
    "    os.path.join(folder, file)\n",
    "    for file in os.listdir(folder) if file.startswith(\"test\")] \n",
    "\n",
    "# Create feature extractor\n",
    "extractor = FeatureExtractor(model)\n",
    "\n",
    "# Extract embeddings\n",
    "embeddings_torank = extractor.get_embeddings(image_paths=ranking_image_paths, transform=transform)\n",
    "embeddings_test = extractor.get_embeddings(image_paths=testing_image_path, transform=transform)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_torank[0].unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking_dict = dict()\n",
    "for idx, embedding_torank in enumerate(embeddings_torank):\n",
    "    similarity_score = F.cosine_similarity(embedding_torank, embeddings_test)  \n",
    "    print(f\"Image path: {ranking_image_paths[idx]}, Similarity {float(similarity_score)}\")\n",
    "    ranking_dict[ranking_image_paths[idx]] = float(similarity_score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_ranking_dict = sorted(ranking_dict.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_ranking_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GradCAM for the test image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_path = 'ranking_annotations/test_image.png'\n",
    "original_img, overlay = apply_gradcam(\n",
    "        model=model, \n",
    "        image_path=test_image_path, \n",
    "        transform=transform,\n",
    "    )\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(original_img)\n",
    "plt.title(\"Resized Original Image Pleural Effusion\")\n",
    "plt.axis(\"off\")\n",
    "    \n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(overlay)\n",
    "plt.title(\"Grad-CAM Heatmap Pleual Effusion\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GradCAM for ranked images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sorted_ranking_dict)):\n",
    "\n",
    "    image_path = sorted_ranking_dict[i][0]\n",
    "    \n",
    "    original_img, overlay = apply_gradcam(\n",
    "            model=model, \n",
    "            image_path=image_path, \n",
    "            transform=transform,\n",
    "        )\n",
    "    \n",
    "\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(original_img)\n",
    "    plt.title(\"Resized Original Image Pleural Effusion\")\n",
    "    plt.axis(\"off\")\n",
    "        \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(overlay)\n",
    "    plt.title(\"Grad-CAM Heatmap Pleual Effusion\")\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    \n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ExplainableAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
