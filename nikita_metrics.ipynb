{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import pandas as pd\n",
    "from preprocessing_utils import prepare_dataset, split_train_val, prepare_test_dataset\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from dataset import CheXpertDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "from training_utils import train_model, upload_pretrained, upload_pretrained_vit\n",
    "from torchsummary import summary\n",
    "import torchvision.models as models\n",
    "from tqdm.notebook import tqdm\n",
    "import torchvision.models as models\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema', 'Pleural Effusion']\n",
    "policies = ['ones', 'zeroes', 'mixed']\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to 224x224 pixels\n",
    "    transforms.ToTensor(),          # Convert images to PyTorch tensors\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize grayscale images\n",
    "])\n",
    "\n",
    "\n",
    "# Import the validation dataset as final test set\n",
    "test_df = pd.read_csv('CheXpert-v1.0-small/valid.csv')\n",
    "\n",
    "# Prepare the test dataset using the last policy in the list and the defined class names\n",
    "test_image_paths, test_label_matrix = prepare_dataset(test_df, policies[-1], class_names)\n",
    "\n",
    "# Create a DataFrame for the test image paths\n",
    "test_image_paths_df = pd.DataFrame({'path': test_image_paths})\n",
    "\n",
    "# Create a DataFrame for the test labels with the class names as columns\n",
    "test_labels_df = pd.DataFrame(test_label_matrix, columns=class_names)\n",
    "\n",
    "# Combine the test image paths DataFrame and the test labels DataFrame\n",
    "test_df = pd.concat([test_image_paths_df, test_labels_df], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet18 = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "model_resnet = upload_pretrained(resnet18, add_layers=True, n_labels=len(class_names), freeze_layers=True)\n",
    "state_dict = torch.load(\"model_resnet.pth\", map_location=torch.device('cpu'))\n",
    "\n",
    "model_resnet.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate_model(model, test_loader: DataLoader, criterion, device='cuda'):\n",
    "    \"\"\"\n",
    "    Evaluate a trained PyTorch model on a test dataset.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The trained PyTorch model.\n",
    "    - test_loader: DataLoader for the test dataset.\n",
    "    - criterion: The loss function used during training.\n",
    "    - device: Device to evaluate the model on ('cpu' or 'cuda').\n",
    "\n",
    "    Returns:\n",
    "    - test_loss: Average loss on the test dataset.\n",
    "    - test_accuracy: Overall accuracy on the test dataset.\n",
    "    - all_predictions: List of predicted values for all samples.\n",
    "    - all_labels: List of ground-truth labels for all samples.\n",
    "    \"\"\"\n",
    "    # model.to(device)\n",
    "    # model.eval()  # Set the model to evaluation mode\n",
    "    \n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(test_loader, desc=\"Evaluating\", unit=\"batch\"):\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            # Convert outputs to binary predictions (multi-label classification)\n",
    "            predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "            \n",
    "            # Store predictions and labels\n",
    "            all_predictions.append(predicted.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.numel()  # Total elements (samples × labels)\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    test_accuracy = correct / total\n",
    "\n",
    "    # Convert pre`dictions and labels to tensors\n",
    "    all_predictions = torch.cat(all_predictions, dim=0)\n",
    "    all_labels = torch.cat(all_labels, dim=0)\n",
    "\n",
    "    print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "    return test_loss, test_accuracy, all_predictions, all_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CheXpertDataset(test_df, class_names, transform=transform)\n",
    "\n",
    "# Create DataLoader for the test dataset\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 13/13 [00:08<00:00,  1.61batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.5618, Test Accuracy: 0.7228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "test_loss, test_accuracy, predictions, true_labels = evaluate_model(model_resnet, test_loader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 0., 1., 0.])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_labels[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = (predictions > 0.5).int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-label subset accuracy: 0.3465346534653465\n",
      "Precision (samples average): 0.09900990099009901\n",
      "Recall (samples average): 0.03118811881188118\n",
      "F1 Score (samples average): 0.046888260254596886\n",
      "_________________________________________________________________________________________________________\n",
      "Precision (macro average): 0.3\n",
      "Recall (macro average): 0.06726190476190477\n",
      "F1 Score (macro average): 0.08622540250447228\n",
      "_________________________________________________________________________________________________________\n",
      "Precision (micro average): 0.5121951219512195\n",
      "Recall (micro average): 0.07526881720430108\n",
      "F1 Score (micro average): 0.13125\n",
      "_________________________________________________________________________________________________________\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        75\n",
      "           1       0.00      0.00      0.00        66\n",
      "           2       0.00      0.00      0.00        32\n",
      "           3       1.00      0.02      0.05        42\n",
      "           4       0.50      0.31      0.38        64\n",
      "\n",
      "   micro avg       0.51      0.08      0.13       279\n",
      "   macro avg       0.30      0.07      0.09       279\n",
      "weighted avg       0.27      0.08      0.10       279\n",
      " samples avg       0.10      0.03      0.05       279\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    hamming_loss,\n",
    "    classification_report\n",
    ")\n",
    "\n",
    "# Convert predictions to binary values based on a threshold (0.5)\n",
    "predicted_labels = (predictions > 0.5).int()\n",
    "\n",
    "# Convert tensors to numpy arrays for sklearn metrics\n",
    "true_labels_np = true_labels.numpy()\n",
    "predicted_labels_np = predicted_labels.numpy()\n",
    "\n",
    "# 1. Accuracy \n",
    "subset_accuracy = accuracy_score(true_labels_np, predicted_labels_np)\n",
    "print(\"Multi-label subset accuracy:\", subset_accuracy)\n",
    "\n",
    "# 2. Calculate precision, recall, and F1 score using 'samples', 'macro' and 'micro' averaging\n",
    "precision_samples = precision_score(true_labels_np, predicted_labels_np, average='samples', zero_division=0)\n",
    "recall_samples = recall_score(true_labels_np, predicted_labels_np, average='samples', zero_division=0)\n",
    "f1_samples = f1_score(true_labels_np, predicted_labels_np, average='samples', zero_division=0)\n",
    "\n",
    "print(\"Precision (samples average):\", precision_samples)\n",
    "print(\"Recall (samples average):\", recall_samples)\n",
    "print(\"F1 Score (samples average):\", f1_samples)\n",
    "print('_________________________________________________________________________________________________________')\n",
    "\n",
    "\n",
    "precision_macro = precision_score(true_labels_np, predicted_labels_np, average='macro', zero_division=0)\n",
    "recall_macro = recall_score(true_labels_np, predicted_labels_np, average='macro', zero_division=0)\n",
    "f1_macro = f1_score(true_labels_np, predicted_labels_np, average='macro', zero_division=0)\n",
    "\n",
    "print(\"Precision (macro average):\", precision_macro)\n",
    "print(\"Recall (macro average):\", recall_macro)\n",
    "print(\"F1 Score (macro average):\", f1_macro)\n",
    "print('_________________________________________________________________________________________________________')\n",
    "\n",
    "\n",
    "precision_micro = precision_score(true_labels_np, predicted_labels_np, average='micro', zero_division=0)\n",
    "recall_micro = recall_score(true_labels_np, predicted_labels_np, average='micro', zero_division=0)\n",
    "f1_micro = f1_score(true_labels_np, predicted_labels_np, average='micro', zero_division=0)\n",
    "\n",
    "print(\"Precision (micro average):\", precision_micro)\n",
    "print(\"Recall (micro average):\", recall_micro)\n",
    "print(\"F1 Score (micro average):\", f1_micro)\n",
    "print('_________________________________________________________________________________________________________')\n",
    "\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(true_labels_np, predicted_labels_np, zero_division=0))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
